{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import types\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from rllab.policies.categorical_gru_policy import CategoricalGRUPolicy\n",
    "from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "from rllab.algos.trpo import TRPO\n",
    "from rllab.misc.overrides import overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rc('savefig', dpi=300)\n",
    "mpl.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_const_delay(d):\n",
    "  return (lambda: d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StudentEnv(gym.Env):\n",
    "  \n",
    "  def __init__(self, n_items=10, n_steps=100, discount=1., sample_delay=None, reward_func='likelihood'):\n",
    "    if sample_delay is None:\n",
    "      self.sample_delay = sample_const_delay(1)\n",
    "    else:\n",
    "      self.sample_delay = sample_delay\n",
    "    self.curr_step = None\n",
    "    self.n_steps = n_steps\n",
    "    self.n_items = n_items\n",
    "    self.now = 0\n",
    "    self.curr_item = 0\n",
    "    self.curr_outcome = None\n",
    "    self.curr_delay = None\n",
    "    self.discount = discount\n",
    "    self.reward_func = reward_func\n",
    "    \n",
    "    self.action_space = spaces.Discrete(n_items)\n",
    "    self.observation_space = spaces.Box(np.zeros(4), np.array([n_items-1, 1, sys.maxsize, sys.maxsize]))\n",
    "    \n",
    "  def _recall_likelihoods(self):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "  def _recall_log_likelihoods(self, eps=1e-9):\n",
    "    return np.log(eps + self._recall_likelihoods())\n",
    "  \n",
    "  def _update_model(self, item, outcome, timestamp, delay):\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def _obs(self):\n",
    "    timestamp = self.now - self.curr_delay\n",
    "    return np.array([self.curr_item, self.curr_outcome, timestamp, self.curr_delay], dtype=int)\n",
    "  \n",
    "  def _rew(self):\n",
    "    if self.reward_func == 'likelihood':\n",
    "      return self._recall_likelihoods().mean()\n",
    "    elif self.reward_func == 'log_likelihood':\n",
    "      return self._recall_log_likelihoods().mean()\n",
    "    else:\n",
    "      raise ValueError\n",
    "  \n",
    "  def _step(self, action):\n",
    "    if self.curr_step is None or self.curr_step >= self.n_steps:\n",
    "      raise ValueError\n",
    "      \n",
    "    if action < 0 or action >= self.n_items:\n",
    "      raise ValueError\n",
    "      \n",
    "    self.curr_item = action\n",
    "    self.curr_outcome = 1 if np.random.random() < self._recall_likelihoods()[action] else 0\n",
    "        \n",
    "    self.curr_step += 1\n",
    "    self.curr_delay = self.sample_delay()\n",
    "    self.now += self.curr_delay\n",
    "    \n",
    "    self._update_model(self.curr_item, self.curr_outcome, self.now, self.curr_delay)\n",
    "    \n",
    "    obs = self._obs()\n",
    "    r = self._rew()\n",
    "    done = self.curr_step == self.n_steps\n",
    "    info = {}\n",
    "    \n",
    "    return obs, r, done, info\n",
    "    \n",
    "  def _reset(self):\n",
    "    self.curr_step = 0\n",
    "    self.now = 0\n",
    "    return self._step(np.random.choice(range(self.n_items)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_difficulty_mean = 1\n",
    "item_difficulty_std = 1\n",
    "\n",
    "log_item_decay_exp_mean = 1\n",
    "log_item_decay_exp_std = 1\n",
    "\n",
    "log_delay_coef_mean = 0\n",
    "log_delay_coef_std = 0.01\n",
    "\n",
    "def sample_item_difficulties(n_items):\n",
    "  return np.random.normal(item_difficulty_mean, item_difficulty_std, n_items)\n",
    "\n",
    "def sample_student_ability():\n",
    "  return 0\n",
    "\n",
    "def sample_window_cw(n_windows):\n",
    "  x = 1 / (np.arange(1, n_windows+1, 1))**2\n",
    "  return x[::-1]\n",
    "  \n",
    "def sample_window_nw(n_windows):\n",
    "  x = 1 / (np.arange(1, n_windows+1, 1))**2\n",
    "  return x[::-1]\n",
    "\n",
    "def sample_item_decay_exps(n_items):\n",
    "  return np.exp(np.random.normal(log_item_decay_exp_mean, log_item_decay_exp_std, n_items))\n",
    "\n",
    "def sample_student_decay_exp():\n",
    "  return 0\n",
    "\n",
    "def sample_delay_coef():\n",
    "  return np.exp(np.random.normal(log_delay_coef_mean, log_delay_coef_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DASHEnv(StudentEnv):\n",
    "  \n",
    "  def __init__(\n",
    "    self, n_windows=5, item_difficulties=None, student_ability=None,\n",
    "    window_cw=None, window_nw=None, item_decay_exps=None, student_decay_exp=None, \n",
    "    delay_coef=None, **kwargs):\n",
    "    super(DASHEnv, self).__init__(**kwargs)\n",
    "    \n",
    "    if item_difficulties is None:\n",
    "      self.item_difficulties = sample_item_difficulties(self.n_items)\n",
    "    else:\n",
    "      if len(item_difficulties) != self.n_items:\n",
    "        raise ValueError\n",
    "      self.item_difficulties = item_difficulties\n",
    "      \n",
    "    if student_ability is None:\n",
    "      self.student_ability = sample_student_ability()\n",
    "    else:\n",
    "      self.student_ability = student_ability\n",
    "      \n",
    "    if item_decay_exps is None:\n",
    "      self.item_decay_exps = sample_item_decay_exps(self.n_items)\n",
    "    else:\n",
    "      if len(item_decay_exps) != self.n_items:\n",
    "        raise ValueError\n",
    "      self.item_decay_exps = item_decay_exps\n",
    "    \n",
    "    if student_decay_exp is None:\n",
    "      self.student_decay_exp = sample_student_decay_exp()\n",
    "    else:\n",
    "      self.student_decay_exp = student_decay_exp\n",
    "      \n",
    "    if delay_coef is None:\n",
    "      self.delay_coef = sample_delay_coef()\n",
    "    else:\n",
    "      self.delay_coef = delay_coef\n",
    "            \n",
    "    if self.n_steps % n_windows != 0:\n",
    "      raise ValueError\n",
    "    self.n_windows = n_windows\n",
    "    self.window_size = self.n_steps // self.n_windows\n",
    "    self.n_correct = None\n",
    "    self.n_attempts = None\n",
    "    \n",
    "    if window_cw is None:\n",
    "      window_cw = sample_window_cw(self.n_windows)\n",
    "    if window_nw is None:\n",
    "      window_nw = sample_window_nw(self.n_windows)\n",
    "    if len(window_cw) != n_windows or len(window_nw) != n_windows:\n",
    "      raise ValueError\n",
    "    \n",
    "    self.window_cw = np.tile(window_cw, self.n_items).reshape((self.n_items, self.n_windows))\n",
    "    self.window_nw = np.tile(window_nw, self.n_items).reshape((self.n_items, self.n_windows))\n",
    "    \n",
    "    self.init_tlasts = np.exp(np.random.normal(0, 0.01, self.n_items))\n",
    "    self._init_params()\n",
    "    \n",
    "  def _init_params(self):\n",
    "    self.n_correct = np.zeros((self.n_items, self.n_windows))\n",
    "    self.n_attempts = np.zeros((self.n_items, self.n_windows))\n",
    "    #self.tlasts = np.ones(self.n_items) * -sys.maxsize\n",
    "    self.tlasts = copy.deepcopy(self.init_tlasts)\n",
    "    \n",
    "  def _current_window(self):\n",
    "    return min(self.n_windows - 1, self.curr_step // self.window_size)\n",
    "    \n",
    "  def _recall_likelihoods(self):\n",
    "    curr_window = self._current_window()\n",
    "    study_histories = (self.window_cw[:, :curr_window]*np.log(\n",
    "      1 + self.n_correct[:, :curr_window]) + self.window_nw[:, :curr_window]*np.log(\n",
    "      1 + self.n_attempts[:, :curr_window])).sum(axis=1)\n",
    "    m = 1 / (1 + np.exp(-(self.student_ability - self.item_difficulties + study_histories)))\n",
    "    f = np.exp(self.student_decay_exp - self.item_decay_exps)\n",
    "    delays = self.now - self.tlasts\n",
    "    return m / (1 + self.delay_coef * delays)**f\n",
    "  \n",
    "  def _update_model(self, item, outcome, timestamp, delay):\n",
    "    curr_window = self._current_window()\n",
    "    if outcome == 1:\n",
    "      self.n_correct[item, curr_window] += 1\n",
    "    self.n_attempts[item, curr_window] += 1\n",
    "    self.tlasts[item] = timestamp\n",
    "    \n",
    "  def _reset(self):\n",
    "    self._init_params()\n",
    "    return super(DASHEnv, self)._reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_item_decay_rates(n_items):\n",
    "  return np.exp(np.random.normal(np.log(0.077), 1, n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EFCEnv(StudentEnv):\n",
    "  '''exponential forgetting curve'''\n",
    "  \n",
    "  def __init__(self, item_decay_rates=None, **kwargs):\n",
    "    super(EFCEnv, self).__init__(**kwargs)\n",
    "    \n",
    "    if item_decay_rates is None:\n",
    "      self.item_decay_rates = sample_item_decay_rates(self.n_items)\n",
    "    else:\n",
    "      self.item_decay_rates = item_decay_rates\n",
    "    \n",
    "    self.tlasts = None\n",
    "    self.strengths = None\n",
    "    self.init_tlasts = np.exp(np.random.normal(0, 1, self.n_items))\n",
    "    self._init_params()\n",
    "    \n",
    "  def _init_params(self):\n",
    "    #self.tlasts = np.ones(self.n_items) * -sys.maxsize\n",
    "    self.tlasts = copy.deepcopy(self.init_tlasts)\n",
    "    self.strengths = np.ones(self.n_items)\n",
    "    \n",
    "  def _recall_likelihoods(self):\n",
    "    return np.exp(-self.item_decay_rates*(self.now - self.tlasts)/self.strengths)\n",
    "  \n",
    "  def _update_model(self, item, outcome, timestamp, delay):\n",
    "    #self.strengths[item] = max(1, self.strengths[item] + 2 * outcome - 1) # fictional Leitner system\n",
    "    self.strengths[item] += 1 # num attempts\n",
    "    self.tlasts[item] = timestamp\n",
    "    \n",
    "  def _reset(self):\n",
    "    self._init_params()\n",
    "    return super(EFCEnv, self)._reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_loglinear_coeffs(n_items):\n",
    "  coeffs = np.array([1, 1, 0])\n",
    "  coeffs = np.concatenate((coeffs, np.random.normal(0, 1, n_items)))\n",
    "  return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HLREnv(StudentEnv):\n",
    "  '''exponential forgetting curve with log-linear memory strength'''\n",
    "  \n",
    "  def __init__(self, loglinear_coeffs=None, **kwargs):\n",
    "    super(HLREnv, self).__init__(**kwargs)\n",
    "    \n",
    "    if loglinear_coeffs is None:\n",
    "      self.loglinear_coeffs = sample_loglinear_coeffs(self.n_items)\n",
    "    else:\n",
    "      self.loglinear_coeffs = loglinear_coeffs\n",
    "    assert self.loglinear_coeffs.size == 3 + self.n_items\n",
    "          \n",
    "    self.tlasts = None\n",
    "    self.loglinear_feats = None\n",
    "    self.init_tlasts = np.exp(np.random.normal(0, 1, self.n_items))\n",
    "    self._init_params()\n",
    "    \n",
    "  def _init_params(self):\n",
    "    #self.tlasts = np.ones(self.n_items) * -sys.maxsize\n",
    "    self.tlasts = copy.deepcopy(self.init_tlasts)\n",
    "    self.loglinear_feats = np.zeros((self.n_items, 3)) # n_attempts, n_correct, n_incorrect\n",
    "    self.loglinear_feats = np.concatenate((self.loglinear_feats, np.eye(self.n_items)), axis=1)\n",
    "    \n",
    "  def _strengths(self):\n",
    "    return np.exp(np.einsum('j,ij->i', self.loglinear_coeffs, self.loglinear_feats))\n",
    "    \n",
    "  def _recall_likelihoods(self):\n",
    "    return np.exp(-(self.now - self.tlasts)/self._strengths())\n",
    "  \n",
    "  def _update_model(self, item, outcome, timestamp, delay):\n",
    "    self.loglinear_feats[item, 0] += 1\n",
    "    self.loglinear_feats[item, 1 if outcome == 1 else 2] += 1\n",
    "    self.tlasts[item] = timestamp\n",
    "    \n",
    "  def _reset(self):\n",
    "    self._init_params()\n",
    "    return super(HLREnv, self)._reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = lambda x: x / x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tutor(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def _next_item(self):\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def _update(self, item, outcome, timestamp, delay):\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def act(self, obs):\n",
    "    self._update(*list(obs))\n",
    "    return self._next_item()\n",
    "  \n",
    "  def learn(self, r):\n",
    "    pass\n",
    "  \n",
    "  def train(self, env, n_eps=10):\n",
    "    return run_eps(self, env, n_eps=n_eps)\n",
    "  \n",
    "  def reset(self):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandTutor(Tutor):\n",
    "  '''sample item uniformly at random'''\n",
    "  \n",
    "  def __init__(self, n_items, init_timestamp=0):\n",
    "    self.n_items = n_items\n",
    "    \n",
    "  def _next_item(self):\n",
    "    return np.random.choice(range(self.n_items))\n",
    "  \n",
    "  def _update(self, item, outcome, timestamp, delay):\n",
    "    pass\n",
    "  \n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeitnerTutor(Tutor):\n",
    "  '''sample item from an infinite leitner queue network'''\n",
    "  \n",
    "  \n",
    "  def __init__(self, n_items, init_timestamp=0, arrival_prob=0.1):\n",
    "    self.arrival_prob = arrival_prob\n",
    "    self.n_items = n_items\n",
    "    self.queues = None\n",
    "    self.curr_q = None\n",
    "    self.curr_item = None\n",
    "    self.just_reset = False\n",
    "    self.reset()\n",
    "        \n",
    "  def _next_item(self):    \n",
    "    if self.curr_item is not None:\n",
    "      raise ValueError\n",
    "      \n",
    "    n_queues = len(self.queues)\n",
    "    q_sampling_rates = 1 / np.sqrt(np.arange(1, n_queues, 1))\n",
    "    q_sampling_rates = np.array([x if not self.queues[i+1].empty() else 0 for i, x in enumerate(q_sampling_rates)])\n",
    "    arrival_prob = self.arrival_prob if not self.queues[0].empty() else 0\n",
    "    q_sampling_rates = np.concatenate((np.array([arrival_prob]), normalize(q_sampling_rates) * (1 - arrival_prob)))\n",
    "    p = normalize(q_sampling_rates)\n",
    "    \n",
    "    if self.queues[0].qsize() == self.n_items: # no items have been shown yet\n",
    "      self.curr_q = 0\n",
    "    else:\n",
    "      self.curr_q = np.random.choice(range(n_queues), p=p)\n",
    "    self.curr_item = self.queues[self.curr_q].get(False)\n",
    "    return self.curr_item\n",
    "    \n",
    "  def _update(self, item, outcome, timestamp, delay):\n",
    "    if not self.just_reset and (self.curr_item is None or item != self.curr_item):\n",
    "      raise ValueError\n",
    "      \n",
    "    if self.just_reset:\n",
    "      for i in range(self.n_items):\n",
    "        if i != item:\n",
    "          self.queues[0].put(i)\n",
    "      \n",
    "    next_q = max(1, self.curr_q + 2 * int(outcome) - 1)\n",
    "    if next_q == len(self.queues):\n",
    "      self.queues.append(Queue())\n",
    "    self.queues[next_q].put(item)\n",
    "    \n",
    "    self.curr_item = None\n",
    "    self.curr_q = None\n",
    "    self.just_reset = False\n",
    "    \n",
    "  def reset(self):\n",
    "    self.queues = [Queue()]\n",
    "      \n",
    "    self.curr_item = None\n",
    "    self.curr_q = 0\n",
    "    self.just_reset = True\n",
    "    \n",
    "  def train(self, gym_env, n_eps=10):\n",
    "    arrival_probs = np.arange(0, 1, 0.01)\n",
    "    n_eps_per_aprob = n_eps // arrival_probs.size\n",
    "    assert n_eps_per_aprob > 0\n",
    "    best_reward = None\n",
    "    best_aprob = None\n",
    "    for aprob in arrival_probs:\n",
    "      self.arrival_prob = aprob\n",
    "      reward = np.mean(run_eps(self, env, n_eps=n_eps_per_aprob))\n",
    "      if best_reward is None or reward > best_reward:\n",
    "        best_aprob = aprob\n",
    "        best_reward = reward\n",
    "    self.arrival_prob = best_aprob\n",
    "    return run_eps(self, env, n_eps=n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThresholdTutor(Tutor):\n",
    "  '''review item with recall likelihood closest to some threshold'''\n",
    "  \n",
    "  def __init__(self, n_items, env, init_timestamp=0):\n",
    "    self.n_items = n_items\n",
    "    self.threshold = None\n",
    "    self.env = copy.deepcopy(env)\n",
    "    self.env._reset()\n",
    "  \n",
    "  def _next_item(self):\n",
    "    return np.argmin(np.abs(self.env._recall_likelihoods() - self.threshold))\n",
    "  \n",
    "  def _update(self, item, outcome, timestamp, delay):\n",
    "    self.env._update_model(item, outcome, timestamp, delay)\n",
    "    self.env.curr_step += 1\n",
    "    self.env.now += delay\n",
    "    \n",
    "  def reset(self):\n",
    "    self.env._reset()\n",
    "  \n",
    "  def train(self, env, n_eps=10):\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "    n_eps_per_thresh = n_eps // thresholds.size\n",
    "    assert n_eps_per_thresh > 0\n",
    "    best_reward = None\n",
    "    best_thresh = None\n",
    "    for thresh in thresholds:\n",
    "      self.threshold = thresh\n",
    "      reward = np.mean(run_eps(self, env, n_eps=n_eps_per_thresh))\n",
    "      if best_reward is None or reward > best_reward:\n",
    "        best_thresh = thresh\n",
    "        best_reward = reward\n",
    "    self.threshold = best_thresh\n",
    "    return run_eps(self, env, n_eps=n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_rl_student_env(env):\n",
    "  env = copy.deepcopy(env)\n",
    "\n",
    "  env.n_item_feats = int(np.log(2 * env.n_items))\n",
    "  env.item_feats = np.random.normal(\n",
    "    np.zeros(2*env.n_items*env.n_item_feats), \n",
    "    np.ones(2*env.n_items*env.n_item_feats)).reshape((2*env.n_items, env.n_item_feats))\n",
    "  env.observation_space = spaces.Box(\n",
    "    np.concatenate((np.ones(env.n_item_feats) * -sys.maxsize, np.zeros(3))),\n",
    "    np.concatenate((np.ones(env.n_item_feats + 2) * sys.maxsize, np.ones(1)))\n",
    "  )\n",
    "\n",
    "  def encode_item(self, item, outcome):\n",
    "    return self.item_feats[self.n_items*outcome+item, :]\n",
    "\n",
    "  def encode_delay(self, delay, outcome):\n",
    "    v = np.zeros(2)\n",
    "    v[outcome] = np.log(1+delay)\n",
    "    return v\n",
    "\n",
    "  def vectorize_obs(self, item, outcome, delay):\n",
    "    return np.concatenate((self.encode_item(item, outcome), self.encode_delay(delay, outcome), np.array([outcome])))\n",
    "\n",
    "  env._obs_orig = env._obs\n",
    "  def _obs(self):\n",
    "    item, outcome, timestamp, delay = env._obs_orig()\n",
    "    return self.vectorize_obs(item, outcome, delay)\n",
    "  \n",
    "  env.encode_item = types.MethodType(encode_item, env)\n",
    "  env.encode_delay = types.MethodType(encode_delay, env)\n",
    "  env.vectorize_obs = types.MethodType(vectorize_obs, env)\n",
    "  env._obs = types.MethodType(_obs, env)\n",
    "\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rllab.envs.gym_env import *\n",
    "\n",
    "class MyGymEnv(GymEnv):\n",
    "    def __init__(self, env, record_video=False, video_schedule=None, log_dir=None, record_log=False,\n",
    "                 force_reset=False):\n",
    "        if log_dir is None:\n",
    "            if logger.get_snapshot_dir() is None:\n",
    "                logger.log(\"Warning: skipping Gym environment monitoring since snapshot_dir not configured.\")\n",
    "            else:\n",
    "                log_dir = os.path.join(logger.get_snapshot_dir(), \"gym_log\")\n",
    "        Serializable.quick_init(self, locals())\n",
    "\n",
    "        self.env = env\n",
    "        self.env_id = ''\n",
    "\n",
    "        assert not (not record_log and record_video)\n",
    "\n",
    "        if log_dir is None or record_log is False:\n",
    "            self.monitoring = False\n",
    "        else:\n",
    "            if not record_video:\n",
    "                video_schedule = NoVideoSchedule()\n",
    "            else:\n",
    "                if video_schedule is None:\n",
    "                    video_schedule = CappedCubicVideoSchedule()\n",
    "            self.env = gym.wrappers.Monitor(self.env, log_dir, video_callable=video_schedule, force=True)\n",
    "            self.monitoring = True\n",
    "\n",
    "        self._observation_space = convert_gym_space(env.observation_space)\n",
    "        logger.log(\"observation space: {}\".format(self._observation_space))\n",
    "        self._action_space = convert_gym_space(env.action_space)\n",
    "        logger.log(\"action space: {}\".format(self._action_space))\n",
    "        self._horizon = self.env.n_steps\n",
    "        self._log_dir = log_dir\n",
    "        self._force_reset = force_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyTutor(Tutor):\n",
    "  \n",
    "  def __init__(self, policy):\n",
    "    self.policy = policy\n",
    "    \n",
    "  def act(self, obs):\n",
    "    return self.policy(obs)\n",
    "  \n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LoggedTRPO(TRPO):  \n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(LoggedTRPO, self).__init__(*args, **kwargs)\n",
    "    self.rew_chkpts = []\n",
    "\n",
    "  @overrides\n",
    "  def train(self):\n",
    "    self.start_worker()\n",
    "    self.init_opt()\n",
    "    for itr in range(self.current_itr, self.n_itr):\n",
    "      paths = self.sampler.obtain_samples(itr)\n",
    "      samples_data = self.sampler.process_samples(itr, paths)\n",
    "      self.optimize_policy(itr, samples_data)\n",
    "      my_policy = lambda obs: self.policy.get_action(obs)[0]\n",
    "      r, _ = run_ep(DummyTutor(my_policy), self.env)\n",
    "      self.rew_chkpts.append(r)\n",
    "      print(self.rew_chkpts[-1])\n",
    "    self.shutdown_worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RLTutor(Tutor):\n",
    "  \n",
    "  def __init__(self, n_items, init_timestamp=0):\n",
    "    self.raw_policy = None\n",
    "    self.curr_obs = None\n",
    "  \n",
    "  def train(self, gym_env, n_eps=10):\n",
    "    env = MyGymEnv(gym_env)\n",
    "    policy = CategoricalGRUPolicy(\n",
    "      env_spec=env.spec, hidden_dim=32,\n",
    "      state_include_action=False)\n",
    "    self.raw_policy = LoggedTRPO(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        baseline=LinearFeatureBaseline(env_spec=env.spec),\n",
    "        batch_size=4000,\n",
    "        max_path_length=env.env.n_steps,\n",
    "        n_itr=n_eps,\n",
    "        discount=0.99,\n",
    "        step_size=0.01,\n",
    "        verbose=False\n",
    "    )\n",
    "    self.raw_policy.train()\n",
    "    return self.raw_policy.rew_chkpts\n",
    "    \n",
    "  def reset(self):\n",
    "    self.curr_obs = None\n",
    "    self.raw_policy.reset()\n",
    "  \n",
    "  def _next_item(self):\n",
    "    if self.curr_obs is None:\n",
    "      raise ValueError\n",
    "    return self.raw_policy.get_action(self.curr_obs)[0]\n",
    "  \n",
    "  def _update(self, obs):\n",
    "    self.curr_obs = self.vectorize_obs(obs)\n",
    "    \n",
    "  def act(self, obs):\n",
    "    self._update(obs)\n",
    "    return self._next_item()\n",
    "  \n",
    "  def reset(self):\n",
    "    self.raw_policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# based on SM2_mnemosyne.py <Peter.Bienstman@UGent.be>\n",
    "#\n",
    "\n",
    "HOUR = 60 * 60 # Seconds in an hour.\n",
    "DAY = 24 * HOUR # Seconds in a day.\n",
    "\n",
    "class Card(object):\n",
    "  \n",
    "    def __init__(self, _id):\n",
    "        self.grade = 0\n",
    "        self.next_rep = 0\n",
    "        self.last_rep = 0\n",
    "        self.easiness = 0\n",
    "        self.acq_reps = 0\n",
    "        self.acq_reps_since_lapse = 0\n",
    "        self.ret_reps = 0\n",
    "        self.ret_reps_since_lapse = 0\n",
    "        self.lapses = 0\n",
    "        self._id = _id\n",
    "\n",
    "class SuperMnemoTutor(Tutor):\n",
    "\n",
    "    \"\"\"Scheduler based on http://www.supermemo.com/english/ol/sm2.htm.\n",
    "    Note that all intervals are in seconds, since time is stored as\n",
    "    integer POSIX timestamps.\n",
    "\n",
    "    Since the scheduling granularity is days, all cards due on the same time\n",
    "    should become due at the same time. In order to keep the SQL query\n",
    "    efficient, we do this by setting 'next_rep' the same for all cards that\n",
    "    are due on the same day.\n",
    "\n",
    "    In order to allow for the fact that the timezone and 'day_starts_at' can\n",
    "    change after scheduling a card, we store 'next_rep' as midnight UTC, and\n",
    "    bring local time and 'day_starts_at' only into play when querying the\n",
    "    database.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_items, init_timestamp=0, non_memorised_cards_in_hand=10, fail_grade=0, pass_grade=2):\n",
    "        self.non_memorised_cards_in_hand = non_memorised_cards_in_hand\n",
    "        self.fail_grade = fail_grade\n",
    "        self.pass_grade = pass_grade\n",
    "        self.state = 1\n",
    "        self.curr_step = 0\n",
    "        self.card_of_id = [Card(i) for i in range(n_items)]\n",
    "        self.curr_item = None\n",
    "        self.now = init_timestamp\n",
    "        self.unseen = set(range(n_items))\n",
    "        self._card_ids_memorised = []\n",
    "        self.reset()\n",
    "        self.n_items = n_items\n",
    "\n",
    "    def true_scheduled_interval(self, card):\n",
    "\n",
    "        \"\"\"Since 'next_rep' is always midnight UTC for retention reps, we need\n",
    "        to take timezone and 'day_starts_at' into account to calculate the\n",
    "        true scheduled interval when we are doing the actual repetition.\n",
    "        This basically undoes the operations from 'adjusted_now'.\n",
    "        Note that during the transition between different timezones, this is\n",
    "        not well-defined, but the influence on the scheduler will be minor\n",
    "        anyhow.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        interval = card.next_rep - card.last_rep\n",
    "        if card.grade < 2:\n",
    "            return interval\n",
    "        interval += HOUR\n",
    "        return int(interval)\n",
    "\n",
    "    def reset(self, new_only=False):\n",
    "\n",
    "        \"\"\"'_card_ids_in_queue' contains the _ids of the cards making up the\n",
    "        queue.\n",
    "\n",
    "        The corresponding fact._ids are also stored in '_fact_ids_in_queue',\n",
    "        which is needed to make sure that no sister cards can be together in\n",
    "        the queue at any time.\n",
    "\n",
    "        '_fact_ids_memorised' has a different function and persists over the\n",
    "        different stages invocations of 'rebuild_queue'. It can be used to\n",
    "        control whether or not memorising a card will prevent a sister card\n",
    "        from being pulled out of the 'unseen' pile, even after the queue has\n",
    "        been rebuilt.\n",
    "\n",
    "        '_card_id_last' is stored to avoid showing the same card twice in a\n",
    "        row.\n",
    "\n",
    "        'stage' stores the stage of the queue building, and is used to skip\n",
    "        over unnecessary queries.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._card_ids_in_queue = []\n",
    "        self._card_id_last = None\n",
    "        self.new_only = new_only\n",
    "        if self.new_only == False:\n",
    "            self.stage = 1\n",
    "        else:\n",
    "            self.stage = 3\n",
    "\n",
    "    def set_initial_grade(self, cards, grade):\n",
    "\n",
    "        \"\"\"Sets the initial grades for a set of sister cards, making sure\n",
    "        their next repetitions do no fall on the same day.\n",
    "\n",
    "        Note that even if the initial grading happens when adding a card, it\n",
    "        is seen as a repetition.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        new_interval = self.calculate_initial_interval(grade)\n",
    "        new_interval += self.calculate_interval_noise(new_interval)\n",
    "        last_rep = self.now\n",
    "        next_rep = last_rep + new_interval\n",
    "        for card in cards:\n",
    "            card.grade = grade\n",
    "            card.easiness = 2.5\n",
    "            card.acq_reps = 1\n",
    "            card.acq_reps_since_lapse = 1\n",
    "            card.last_rep = last_rep\n",
    "            card.next_rep = next_rep\n",
    "            next_rep += DAY\n",
    "\n",
    "    def calculate_initial_interval(self, grade):\n",
    "\n",
    "        \"\"\"The first repetition is treated specially, and gives longer\n",
    "        intervals, to allow for the fact that the user may have seen this\n",
    "        card before.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return (0, 0, 1*DAY, 3*DAY, 4*DAY, 7*DAY) [grade]\n",
    "\n",
    "    def calculate_interval_noise(self, interval):\n",
    "        if interval == 0:\n",
    "            noise = 0\n",
    "        elif interval <= 10 * DAY:\n",
    "            noise = random.choice([0, DAY])\n",
    "        elif interval <= 60 * DAY:\n",
    "            noise = random.uniform(-3 * DAY, 3 * DAY)\n",
    "        else:\n",
    "            noise = random.uniform(-0.05 * interval, 0.05 * interval)\n",
    "        return int(noise)\n",
    "      \n",
    "    def cards_due_for_ret_rep(self):\n",
    "        return sorted(\n",
    "          range(self.n_items),\n",
    "          key=lambda i: self.card_of_id[i].next_rep - self.card_of_id[i].last_rep,\n",
    "          reverse=True)\n",
    "        \n",
    "    def cards_to_relearn(self, grade=0):\n",
    "        # TODO: only return cards incorrectly answered in stage 1\n",
    "        return [i for i in range(self.n_items) if self.card_of_id[i].grade == grade and i not in self.unseen]\n",
    "        \n",
    "    def cards_new_memorising(self, grade=0):\n",
    "        return [i for i in range(self.n_items) if self.card_of_id[i].grade == grade and i not in self.unseen]\n",
    "        \n",
    "    def cards_unseen(self, limit=50):\n",
    "        return random.sample(self.unseen, limit) if limit < len(self.unseen) else self.unseen\n",
    "        \n",
    "    def card(self, card_id):\n",
    "        return self.card_of_id[card_id]\n",
    "      \n",
    "    def interval_multiplication_factor(self, *args):\n",
    "        return 1\n",
    "\n",
    "    def rebuild_queue(self, learn_ahead=False):\n",
    "        self._card_ids_in_queue = []\n",
    "\n",
    "        # Stage 1\n",
    "        #\n",
    "        # Do the cards that are scheduled for today (or are overdue), but\n",
    "        # first do those that have the shortest interval, as being a day\n",
    "        # late on an interval of 2 could be much worse than being a day late\n",
    "        # on an interval of 50.\n",
    "        # Fetch maximum 50 cards at the same time, as a trade-off between\n",
    "        # memory usage and redoing the query.\n",
    "        if self.stage == 1:\n",
    "            for _card_id in self.cards_due_for_ret_rep():\n",
    "                self._card_ids_in_queue.append(_card_id)\n",
    "            if len(self._card_ids_in_queue):\n",
    "                return\n",
    "            self.stage = 2\n",
    "\n",
    "        # Stage 2\n",
    "        #\n",
    "        # Now rememorise the cards that we got wrong during the last stage.\n",
    "        # Concentrate on only a limited number of non memorised cards, in\n",
    "        # order to avoid too long intervals between repetitions.\n",
    "        limit = self.non_memorised_cards_in_hand\n",
    "        non_memorised_in_queue = 0\n",
    "        if self.stage == 2:\n",
    "            for _card_id in self.cards_to_relearn(grade=1):\n",
    "                if _card_id not in self._card_ids_in_queue:\n",
    "                    if non_memorised_in_queue < limit:\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        non_memorised_in_queue += 1\n",
    "                    if non_memorised_in_queue >= limit:\n",
    "                        break\n",
    "            for _card_id in self.cards_to_relearn(grade=0):\n",
    "                if _card_id not in self._card_ids_in_queue:\n",
    "                    if non_memorised_in_queue < limit:\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        non_memorised_in_queue += 1\n",
    "                    if non_memorised_in_queue >= limit:\n",
    "                        break\n",
    "            random.shuffle(self._card_ids_in_queue)\n",
    "            # Only stop when we reach the non memorised limit. Otherwise, keep\n",
    "            # going to add some extra cards to get more spread.\n",
    "            if non_memorised_in_queue >= limit:\n",
    "                return\n",
    "            # If the queue is empty, we can skip stage 2 in the future.\n",
    "            if len(self._card_ids_in_queue) == 0:\n",
    "                self.stage = 3\n",
    "\n",
    "        # Stage 3\n",
    "        #\n",
    "        # Now do the cards which have never been committed to long-term\n",
    "        # memory, but which we have seen before.\n",
    "        # Use <= in the stage check, such that earlier stages can use\n",
    "        # cards from this stage to increase the hand.\n",
    "        if self.stage <= 3:\n",
    "            for _card_id in self.cards_new_memorising(grade=1):\n",
    "                if _card_id not in self._card_ids_in_queue:\n",
    "                    if non_memorised_in_queue < limit:\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        non_memorised_in_queue += 1\n",
    "                    if non_memorised_in_queue >= limit:\n",
    "                        break\n",
    "            for _card_id in self.cards_new_memorising(grade=0):\n",
    "                if _card_id not in self._card_ids_in_queue:\n",
    "                    if non_memorised_in_queue < limit:\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        non_memorised_in_queue += 1\n",
    "                    if non_memorised_in_queue >= limit:\n",
    "                        break\n",
    "            random.shuffle(self._card_ids_in_queue)\n",
    "            # Only stop when we reach the grade 0 limit. Otherwise, keep\n",
    "            # going to add some extra cards to get more spread.\n",
    "            if non_memorised_in_queue >= limit:\n",
    "                return\n",
    "            # If the queue is empty, we can skip stage 3 in the future.\n",
    "            if len(self._card_ids_in_queue) == 0:\n",
    "                self.stage = 4\n",
    "\n",
    "        # Stage 4\n",
    "        #\n",
    "        # Now add some cards we have yet to see for the first time.\n",
    "        # Use <= in the stage check, such that earlier stages can use\n",
    "        # cards from this stage to increase the hand.\n",
    "        if self.stage <= 4:\n",
    "            # Preferentially keep away from sister cards for as long as\n",
    "            # possible.\n",
    "            for _card_id in self.cards_unseen(limit=min(limit, 50)):\n",
    "                if _card_id not in self._card_ids_in_queue \\\n",
    "                    and _card_id not in self._card_ids_memorised:\n",
    "                    self._card_ids_in_queue.append(_card_id)\n",
    "                    non_memorised_in_queue += 1\n",
    "                    if non_memorised_in_queue >= limit:\n",
    "                        if self.new_only == False:\n",
    "                            self.stage = 2\n",
    "                        else:\n",
    "                            self.stage = 3\n",
    "                        return\n",
    "            # If the queue is close to empty, start pulling in sister cards.\n",
    "            if len(self._card_ids_in_queue) <= 2:\n",
    "                for _card_id in self.cards_unseen(limit=min(limit, 50)):\n",
    "                    if _card_id not in self._card_ids_in_queue:\n",
    "                        self._card_ids_in_queue.append(_card_id)\n",
    "                        non_memorised_in_queue += 1\n",
    "                        if non_memorised_in_queue >= limit:\n",
    "                            if self.new_only == False:\n",
    "                                self.stage = 2\n",
    "                            else:\n",
    "                                self.stage = 3\n",
    "                            return\n",
    "            # If the queue is still empty, go to learn ahead of schedule.\n",
    "            if len(self._card_ids_in_queue) == 0:\n",
    "                self.stage = 5\n",
    "\n",
    "        # Stage 5\n",
    "        #\n",
    "        # If we get to here, there are no more scheduled cards or new cards\n",
    "        # to learn. The user can signal that he wants to learn ahead by\n",
    "        # calling rebuild_queue with 'learn_ahead' set to True.\n",
    "        # Don't shuffle this queue, as it's more useful to review the\n",
    "        # earliest scheduled cards first. We only put 50 cards at the same\n",
    "        # time into the queue, in order to save memory.\n",
    "        if self.new_only == False:\n",
    "            self.stage = 2\n",
    "        else:\n",
    "            self.stage = 3            \n",
    "   \n",
    "    def next_card(self, learn_ahead=False):\n",
    "        # Populate queue if it is empty, and pop first card from the queue.\n",
    "        if len(self._card_ids_in_queue) == 0:\n",
    "            self.rebuild_queue(learn_ahead)\n",
    "            if len(self._card_ids_in_queue) == 0:\n",
    "                return None\n",
    "        _card_id = self._card_ids_in_queue.pop(0)\n",
    "        # Make sure we don't show the same card twice in succession.\n",
    "        if self._card_id_last:\n",
    "            while _card_id == self._card_id_last:\n",
    "                # Make sure we have enough cards to vary, but exit in hopeless\n",
    "                # situations.\n",
    "                if len(self._card_ids_in_queue) == 0:\n",
    "                    self.rebuild_queue(learn_ahead)\n",
    "                    if len(self._card_ids_in_queue) == 0:\n",
    "                        return None\n",
    "                    if set(self._card_ids_in_queue) == set([_card_id]):\n",
    "                        return db.card(_card_id, is_id_internal=True)\n",
    "                _card_id = self._card_ids_in_queue.pop(0)\n",
    "        self._card_id_last = _card_id\n",
    "        return self.card(_card_id)\n",
    "\n",
    "    def _next_item(self):\n",
    "        if self.curr_item is not None:\n",
    "            raise ValueError\n",
    "          \n",
    "        card = self.next_card()\n",
    "        if card is None:\n",
    "            raise ValueError\n",
    "        self.curr_item = card._id\n",
    "        return self.curr_item\n",
    "\n",
    "    def grade_answer(self, card, new_grade, dry_run=False):\n",
    "        # When doing a dry run, make a copy to operate on. This leaves the\n",
    "        # original in the GUI intact.\n",
    "        if dry_run:\n",
    "            card = copy.copy(card)\n",
    "        # Determine whether we learned on time or not (only relevant for\n",
    "        # grades 2 or higher).\n",
    "        if self.now - DAY >= card.next_rep: # Already due yesterday.\n",
    "            timing = \"LATE\"\n",
    "        else:\n",
    "            if self.now < card.next_rep: # Not due today.\n",
    "                timing = \"EARLY\"\n",
    "            else:\n",
    "                timing = \"ON TIME\"\n",
    "        # Calculate the previously scheduled interval, i.e. the interval that\n",
    "        # led up to this repetition.\n",
    "        scheduled_interval = self.true_scheduled_interval(card)\n",
    "        # If we memorise a card, keep track of its fact, so that we can avoid\n",
    "        # pulling a sister card from the 'unseen' pile.\n",
    "        if not dry_run and card.grade < 2 and new_grade >= 2:\n",
    "            self._card_ids_memorised.append(card._id)\n",
    "        if card.grade == -1: # Unseen card.\n",
    "            actual_interval = 0\n",
    "        else:\n",
    "            actual_interval = self.now - card.last_rep\n",
    "        if card.grade == -1:\n",
    "            # The card has not yet been given its initial grade.\n",
    "            card.easiness = 2.5\n",
    "            card.acq_reps = 1\n",
    "            card.acq_reps_since_lapse = 1\n",
    "            new_interval = self.calculate_initial_interval(new_grade)\n",
    "        elif card.grade in [0, 1] and new_grade in [0, 1]:\n",
    "            # In the acquisition phase and staying there.\n",
    "            card.acq_reps += 1\n",
    "            card.acq_reps_since_lapse += 1\n",
    "            new_interval = 0\n",
    "        elif card.grade in [0, 1] and new_grade in [2, 3, 4, 5]:\n",
    "             # In the acquisition phase and moving to the retention phase.\n",
    "             card.acq_reps += 1\n",
    "             card.acq_reps_since_lapse += 1\n",
    "             if new_grade == 2:\n",
    "                 new_interval = DAY\n",
    "             elif new_grade == 3:\n",
    "                 new_interval = random.choice([1, 1, 2]) * DAY\n",
    "             elif new_grade == 4:\n",
    "                 new_interval = random.choice([1, 2, 2]) * DAY\n",
    "             elif new_grade == 5:\n",
    "                 new_interval = 2 * DAY\n",
    "             # Make sure the second copy of a grade 0 card doesn't show\n",
    "             # up again.\n",
    "             if not dry_run and card.grade == 0:\n",
    "                 if card._id in self._card_ids_in_queue:\n",
    "                     self._card_ids_in_queue.remove(card._id)\n",
    "        elif card.grade in [2, 3, 4, 5] and new_grade in [0, 1]:\n",
    "             # In the retention phase and dropping back to the\n",
    "             # acquisition phase.\n",
    "             card.ret_reps += 1\n",
    "             card.lapses += 1\n",
    "             card.acq_reps_since_lapse = 0\n",
    "             card.ret_reps_since_lapse = 0\n",
    "             new_interval = 0\n",
    "        elif card.grade in [2, 3, 4, 5] and new_grade in [2, 3, 4, 5]:\n",
    "            # In the retention phase and staying there.\n",
    "            card.ret_reps += 1\n",
    "            card.ret_reps_since_lapse += 1\n",
    "            # Don't update the easiness when learning ahead.\n",
    "            if timing in [\"LATE\", \"ON TIME\"]:\n",
    "                if new_grade == 2:\n",
    "                    card.easiness -= 0.16\n",
    "                if new_grade == 3:\n",
    "                    card.easiness -= 0.14\n",
    "                if new_grade == 5:\n",
    "                    card.easiness += 0.10\n",
    "                if card.easiness < 1.3:\n",
    "                    card.easiness = 1.3\n",
    "            if card.ret_reps_since_lapse == 1:\n",
    "                new_interval = 6 * DAY\n",
    "            else:\n",
    "                if new_grade == 2 or new_grade == 3:\n",
    "                    if timing in [\"ON TIME\", \"EARLY\"]:\n",
    "                        new_interval = actual_interval * card.easiness\n",
    "                    else:\n",
    "                        # Learning late and interval was too long, so don't\n",
    "                        # increase the interval and use scheduled_interval\n",
    "                        # again as opposed to the much larger\n",
    "                        # actual_interval * card.easiness.\n",
    "                        new_interval = scheduled_interval\n",
    "                if new_grade == 4:\n",
    "                    new_interval = actual_interval * card.easiness\n",
    "                if new_grade == 5:\n",
    "                    if timing in [\"EARLY\"]:\n",
    "                        # Learning ahead and interval was too short. To avoid\n",
    "                        # that the intervals increase explosively when learning\n",
    "                        # ahead, take scheduled_interval as opposed to the\n",
    "                        # much larger actual_interval * card.easiness.\n",
    "                        new_interval = scheduled_interval\n",
    "                    else:\n",
    "                        new_interval = actual_interval * card.easiness\n",
    "                # Pathological case which can occur when learning ahead a card\n",
    "                # in a single card database many times on the same day, such\n",
    "                # that actual_interval becomes 0.\n",
    "                if new_interval < DAY:\n",
    "                    new_interval = DAY\n",
    "        # Allow plugins to modify new_interval by multiplying it.\n",
    "        new_interval *= self.interval_multiplication_factor(card, new_interval)\n",
    "        new_interval = int(new_interval)\n",
    "        # When doing a dry run, stop here and return the scheduled interval.\n",
    "        if dry_run:\n",
    "            return new_interval\n",
    "        # Add some randomness to interval.\n",
    "        new_interval += self.calculate_interval_noise(new_interval)\n",
    "        # Update card properties. 'last_rep' is the time the card was graded,\n",
    "        # not when it was shown.\n",
    "        card.grade = new_grade\n",
    "        card.last_rep = self.now\n",
    "        if new_grade >= 2:\n",
    "            card.next_rep = card.last_rep + new_interval\n",
    "        else:\n",
    "            card.next_rep = card.last_rep\n",
    "       \n",
    "        return new_interval\n",
    "      \n",
    "    def _update(self, item, outcome, timestamp, delay):\n",
    "        if self.curr_step > 0 and (self.curr_item is None or item != self.curr_item):\n",
    "            raise ValueError\n",
    "  \n",
    "        self.now = timestamp\n",
    "        try:\n",
    "          self.unseen.remove(item)\n",
    "        except KeyError:\n",
    "          pass\n",
    "        self.grade_answer(self.card(item), (self.fail_grade, self.pass_grade)[int(outcome)])\n",
    "        \n",
    "        self.curr_item = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_ep(agent, env):\n",
    "  agent.reset()\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  totalr = []\n",
    "  observations = []\n",
    "  while not done:\n",
    "    action = agent.act(obs)\n",
    "    obs, r, done, _ = env.step(action)\n",
    "    agent.learn(r)\n",
    "    totalr.append(r)\n",
    "    observations.append(obs)\n",
    "  return np.mean(totalr), observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_eps(agent, env, n_eps=100):\n",
    "  tot_rew = []\n",
    "  for i in range(n_eps):\n",
    "    totalr, _ = run_ep(agent, env)\n",
    "    tot_rew.append(totalr)\n",
    "  return tot_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "n_items = 30\n",
    "const_delay = 5\n",
    "discount = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_reps = 10\n",
    "n_eps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "  'n_items': n_items, 'n_steps': n_steps, 'discount': discount, \n",
    "  'sample_delay': sample_const_delay(const_delay)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_funcs = ['likelihood', 'log_likelihood']\n",
    "envs = [\n",
    "  ('EFC', EFCEnv), \n",
    "  ('HLR', HLREnv), \n",
    "  ('DASH', DASHEnv)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tutor_builders = [\n",
    "  ('Random', RandTutor),\n",
    "  ('Leitner', LeitnerTutor),\n",
    "  ('SuperMnemo', SuperMnemoTutor),\n",
    "  ('Threshold', ThresholdTutor),\n",
    "  ('RL', RLTutor)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R = np.zeros((len(envs) * len(reward_funcs), len(tutor_builders), n_eps, n_reps))\n",
    "for h, (base_env_name, base_env) in enumerate(envs):\n",
    "  for m, reward_func in enumerate(reward_funcs):\n",
    "    k = h*len(reward_funcs)+m\n",
    "    env_name = base_env_name + '-' + ('L' if reward_func == 'likelihood' else 'LL')\n",
    "    for j in range(n_reps):\n",
    "      env = base_env(**env_kwargs, reward_func=reward_func)\n",
    "      rl_env = make_rl_student_env(env)\n",
    "      for i, (tutor_name, build_tutor) in enumerate(tutor_builders):\n",
    "        if tutor_name.startswith('RL'):\n",
    "          agent = build_tutor(n_items)\n",
    "          R[k, i, :, j] = agent.train(rl_env, n_eps=n_eps)\n",
    "        else:\n",
    "          if 'Thresh' in tutor_name:\n",
    "            agent = build_tutor(n_items, env=env)\n",
    "          else:\n",
    "            agent = build_tutor(n_items)\n",
    "          R[k, i, :, j] = agent.train(env, n_eps=n_eps)\n",
    "        print(env_name, j, tutor_name, np.mean(R[k, i, :, j]))\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_logs = {\n",
    "  'n_steps': n_steps,\n",
    "  'n_items': n_items,\n",
    "  'discount': discount,\n",
    "  'const_delay': const_delay,\n",
    "  'n_reps': n_reps,\n",
    "  'n_eps': n_eps,\n",
    "  'env_names': list(zip(*envs))[0],\n",
    "  'tutor_names': list(zip(*tutor_builders))[0],\n",
    "  'reward_funcs': reward_funcs,\n",
    "  'rewards': R\n",
    "}\n",
    "with open(os.path.join(data_dir, 'reward_logs.pkl'), 'wb') as f:\n",
    "  pickle.dump(reward_logs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((6, len(tutor_builders), n_eps, n_reps))\n",
    "for i in range(6):\n",
    "  with open(os.path.join(data_dir, 'reward_logs.pkl.%d' % i), 'rb') as f:\n",
    "    reward_logs = pickle.load(f)\n",
    "    R[i] = reward_logs['rewards']\n",
    "    print(reward_logs['env_names'], reward_logs['reward_funcs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(R.shape[0]):\n",
    "  for j in range(R.shape[3]):\n",
    "    R[i, :, :, j] = 100 * (R[i, :, :, j] - R[i, 0, :, j]) / abs(R[i, 0, :, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_avg(d, n=5):\n",
    "  s = np.concatenate((np.zeros(1), np.cumsum(d).astype(float)))\n",
    "  return (s[n:] - s[:-n]) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_means = lambda x: np.nanmean(x, axis=1)\n",
    "r_stderrs = lambda x: np.nanstd(x, axis=1) / np.sqrt(np.count_nonzero(x, axis=1))\n",
    "r_mins = lambda x: r_means(x) - r_stderrs(x)#np.nanmin(x, axis=1)\n",
    "r_maxs = lambda x: r_means(x) + r_stderrs(x)#np.nanmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h, (env_name, _) in enumerate(envs):\n",
    "  for m, reward_func in enumerate(reward_funcs):\n",
    "    k = h*len(reward_funcs)+m\n",
    "    for i, (tutor_name, _) in enumerate(tutor_builders):\n",
    "      print(env_name, reward_func, tutor_name, np.nanmean(R[k, i, :, :]), np.nanstd(R[k, i, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_of_env_name = {\n",
    "  'EFC': 'Exponential Forgetting Curve',\n",
    "  'HLR': 'Half-Life Regression',\n",
    "  'DASH': 'Generalized Power-Law'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for h, (env_name, _) in enumerate(envs):\n",
    "  for m, reward_func in enumerate(reward_funcs):\n",
    "    k = h*len(reward_funcs)+m\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Percent better than Random\\n(Reward: %s)' % reward_func.replace('_', '-').replace('likelihood', 'Likelihood').replace('log', 'Log'))\n",
    "    plt.title('Student Model: %s' % title_of_env_name[env_name])\n",
    "\n",
    "    colors = ['gray', 'teal', 'teal', 'teal', 'orange']\n",
    "    styles = ['dotted', 'dashed', 'dashdot', 'solid', 'solid']\n",
    "    for i, (tutor_name, _) in enumerate(tutor_builders):\n",
    "      if tutor_name == 'RL': tutor_name = 'TRPO'\n",
    "      if tutor_name == 'TRPO':\n",
    "        x = range(R.shape[2])\n",
    "        y1 = r_mins(R[k, i, :, :])\n",
    "        y2 = r_maxs(R[k, i, :, :])\n",
    "        plt.fill_between(x, y1, y2, where=y2 >= y1, facecolor=colors[i], interpolate=True, alpha=0.5, label=tutor_name)\n",
    "        plt.plot(r_means(R[k, i, :, :]), color=colors[i])\n",
    "      else:\n",
    "        plt.axhline(y=np.nanmean(R[k, i, :, :]), color=colors[i], linestyle=styles[i], label=tutor_name)\n",
    "    \n",
    "    plt.yticks(plt.yticks()[0], [str(int(x)) + r'\\%' for x in plt.yticks()[0]])\n",
    "        \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(os.path.join(data_dir, '%s-%s.pdf' % (env_name, reward_func)), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
